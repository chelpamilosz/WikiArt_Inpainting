{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "CURRENT_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiArtDataset(Dataset):\n",
    "    def __init__(self, h5_path: str, mask_h5_path: str, csv_path: str, set_type: str, group_filter=None, transform=None, mask_transform=None):\n",
    "        self.h5_path = h5_path\n",
    "        self.mask_h5_path = mask_h5_path\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "        # Load the CSV file\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Filter the dataframe based on the set_type\n",
    "        self.df = self.df[self.df['set_type'] == set_type]\n",
    "\n",
    "        if group_filter is not None:\n",
    "            self.df = self.df[self.df['cluster_label'] == group_filter]\n",
    "        \n",
    "        with h5py.File(self.h5_path, 'r') as h5f:\n",
    "            self.length = len(self.df)\n",
    "        \n",
    "        with h5py.File(self.mask_h5_path, 'r') as mask_h5f:\n",
    "            self.num_masks = mask_h5f['mask'].shape[0]  # Number of masks available\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def _open_hdf5(self):\n",
    "        if not hasattr(self, '_hf') or self._hf is None:\n",
    "            self._hf = h5py.File(self.h5_path, 'r')\n",
    "\n",
    "        if not hasattr(self, '_mask_hf') or self._mask_hf is None:\n",
    "            self._mask_hf = h5py.File(self.mask_h5_path, 'r')\n",
    "\n",
    "    def _get_random_mask(self):\n",
    "        mask_idx = np.random.randint(0, self.num_masks)\n",
    "        mask = self._mask_hf['mask'][mask_idx]\n",
    "        return mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self._open_hdf5()\n",
    "\n",
    "        # Get the index from the dataframe\n",
    "        row = self.df.iloc[idx]\n",
    "        image_idx = row['index']\n",
    "        cluster_label = row['cluster_label']\n",
    "\n",
    "        image = self._hf['image'][image_idx]\n",
    "        image = torch.from_numpy(image).float()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        mask = self._get_random_mask()\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        return image, mask, cluster_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetInpainting(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, use_dropout=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "        # Pre-trained encoder using timm\n",
    "        self.encoder = timm.create_model('resnet34', pretrained=True, features_only=True, out_indices=(0, 1, 2, 3, 4))\n",
    "        encoder_channels = self.encoder.feature_info.channels()  # Channels for each encoder block\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder4 = self.upconv_block(encoder_channels[-1], encoder_channels[-2])\n",
    "        self.decoder3 = self.upconv_block(encoder_channels[-2], encoder_channels[-3])\n",
    "        self.decoder2 = self.upconv_block(encoder_channels[-3], encoder_channels[-4])\n",
    "        self.decoder1 = self.upconv_block(encoder_channels[-4], encoder_channels[-5])\n",
    "\n",
    "        # Final upsampling to 256x256 and output\n",
    "        self.final_upconv = nn.ConvTranspose2d(encoder_channels[-5], 64, kernel_size=2, stride=2)\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Apply the mask to the input image\n",
    "        masked_image = x * (1 - mask)\n",
    "\n",
    "        # Encoder pass\n",
    "        features = self.encoder(masked_image)\n",
    "        e1, e2, e3, e4, e5 = features\n",
    "\n",
    "        # Bottleneck\n",
    "        b = e5\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d4 = self.decoder4(b) + e4\n",
    "        d3 = self.decoder3(d4) + e3\n",
    "        d2 = self.decoder2(d3) + e2\n",
    "        d1 = self.decoder1(d2) + e1\n",
    "\n",
    "        # Final upsampling and output\n",
    "        output = self.final_upconv(d1)\n",
    "        output = self.final_conv(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path, model_class, device='cpu'):\n",
    "    model = model_class()\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(original, masked, mask, output):\n",
    "    original = original.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "    masked = masked.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "    mask = mask.squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "    output = output.permute(1, 2, 0).detach().cpu().numpy().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    ax[0].imshow(original)\n",
    "    ax[0].set_title('Original')\n",
    "    ax[1].imshow(masked)\n",
    "    ax[1].set_title('Masked')\n",
    "    ax[2].imshow(mask, cmap='gray')\n",
    "    ax[2].set_title('Mask')\n",
    "    ax[3].imshow(output)\n",
    "    ax[3].set_title('Output')\n",
    "    for a in ax:\n",
    "        a.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sample(dataset, model, device='cuda'):\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    model.to(device)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        images, masks, label = batch\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        masked_image = images * (1 - masks.unsqueeze(1))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(images, masks.unsqueeze(1))\n",
    "\n",
    "        visualize_results(\n",
    "            original=images[0],\n",
    "            masked=masked_image[0],\n",
    "            mask=masks[0],\n",
    "            output=output[0]\n",
    "        )\n",
    "\n",
    "        print('Cluster label:', label.item())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = {\n",
    "    0 : f'{CURRENT_DIR}\\\\weights\\\\0\\\\best_weights_epoch_46_val_loss_24.9703-4417.pth',\n",
    "    1 : f'{CURRENT_DIR}\\\\weights\\\\1\\\\best_weights_epoch_84_val_loss_20.6325-2379.pth',\n",
    "    2 : f'{CURRENT_DIR}\\\\weights\\\\2\\\\best_weights_epoch_45_val_loss_14.0691-4231.pth',\n",
    "    3 : f'{CURRENT_DIR}\\\\weights\\\\3\\\\best_weights_epoch_97_val_loss_23.3337-1273.pth',\n",
    "    4 : f'{CURRENT_DIR}\\\\weights\\\\4\\\\best_weights_epoch_88_val_loss_25.4863-1512.pth',\n",
    "    5 : f'{CURRENT_DIR}\\\\weights\\\\5\\\\best_weights_epoch_79_val_loss_24.8253-2159.pth',\n",
    "    6 : f'{CURRENT_DIR}\\\\weights\\\\6\\\\best_weights_epoch_73_val_loss_26.2931-2145.pth'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\vscodeProjects\\\\WikiArt_Inpainting\\\\model\\\\inpainting\\\\inpainting\\\\weights\\\\2\\\\best_weights_epoch_45_val_loss_14.0691-4231.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m mask_h5_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvscodeProjects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mWikiArt_Inpainting\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124minpainting\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msquare.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m WikiArtDataset(h5_path, mask_h5_path, annotations_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, group_filter\u001b[38;5;241m=\u001b[39mGROUP_ID)\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(model_paths[GROUP_ID], UNetInpainting)\n\u001b[0;32m     11\u001b[0m evaluate_sample(test_dataset, model, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[81], line 3\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(checkpoint_path, model_class, device)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(checkpoint_path, model_class, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_class()\n\u001b[1;32m----> 3\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\.conda\\envs\\unn\\Lib\\site-packages\\torch\\serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\.conda\\envs\\unn\\Lib\\site-packages\\torch\\serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\.conda\\envs\\unn\\Lib\\site-packages\\torch\\serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\vscodeProjects\\\\WikiArt_Inpainting\\\\model\\\\inpainting\\\\inpainting\\\\weights\\\\2\\\\best_weights_epoch_45_val_loss_14.0691-4231.pth'"
     ]
    }
   ],
   "source": [
    "GROUP_ID = 2\n",
    "\n",
    "h5_path = f'{CURRENT_DIR}\\\\wikiart\\\\dataset.h5'\n",
    "annotations_path = r'C:\\vscodeProjects\\WikiArt_Inpainting\\model\\inpainting\\wikiart\\clustered_full_split.csv'\n",
    "mask_h5_path = r'C:\\vscodeProjects\\WikiArt_Inpainting\\model\\inpainting\\masks\\square.h5'\n",
    "\n",
    "test_dataset = WikiArtDataset(h5_path, mask_h5_path, annotations_path, 'test', group_filter=GROUP_ID)\n",
    "\n",
    "model = load_model(model_paths[GROUP_ID], UNetInpainting)\n",
    "\n",
    "evaluate_sample(test_dataset, model, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\vscodeProjects\\\\WikiArt_Inpainting\\\\model\\\\inpainting'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
