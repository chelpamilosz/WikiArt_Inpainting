{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-24T19:04:39.161608Z",
     "iopub.status.busy": "2025-01-24T19:04:39.161086Z",
     "iopub.status.idle": "2025-01-24T19:04:39.181196Z",
     "shell.execute_reply": "2025-01-24T19:04:39.179831Z",
     "shell.execute_reply.started": "2025-01-24T19:04:39.161564Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/wikiart-damage-masks/square.h5\n",
      "/kaggle/input/wikiart-damage-masks/irregular.h5\n",
      "/kaggle/input/wikiart-inpainting-full-set-weights/best_weights_epoch_28_val_loss_3.7235-2348.pth\n",
      "/kaggle/input/wikiart-cluster-annotations-split/combined_clusters.csv\n",
      "/kaggle/input/wikiart-clean-without-split/annotations.csv\n",
      "/kaggle/input/wikiart-clean-without-split/dataset.h5\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T19:04:39.183348Z",
     "iopub.status.busy": "2025-01-24T19:04:39.182988Z",
     "iopub.status.idle": "2025-01-24T19:04:39.195215Z",
     "shell.execute_reply": "2025-01-24T19:04:39.193868Z",
     "shell.execute_reply.started": "2025-01-24T19:04:39.183314Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip -q install lightning\n",
    "# !pip -q install comet-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T19:04:39.198645Z",
     "iopub.status.busy": "2025-01-24T19:04:39.198231Z",
     "iopub.status.idle": "2025-01-24T19:04:39.210621Z",
     "shell.execute_reply": "2025-01-24T19:04:39.209331Z",
     "shell.execute_reply.started": "2025-01-24T19:04:39.198608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T19:04:39.212564Z",
     "iopub.status.busy": "2025-01-24T19:04:39.212081Z",
     "iopub.status.idle": "2025-01-24T19:04:39.226175Z",
     "shell.execute_reply": "2025-01-24T19:04:39.224888Z",
     "shell.execute_reply.started": "2025-01-24T19:04:39.212388Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This variable is used to identify the group of models that are being trained based on previous clusterization.\n",
    "Clusterization output was combined_clusters.csv with column determining the group of the model.\n",
    "Group was assigned to train and validation sets only.\n",
    "If GROUP_ID is None, the model will be trained on the whole dataset. \n",
    "'''\n",
    "\n",
    "GROUP_ID = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T19:04:39.228068Z",
     "iopub.status.busy": "2025-01-24T19:04:39.227652Z",
     "iopub.status.idle": "2025-01-24T19:04:40.790792Z",
     "shell.execute_reply": "2025-01-24T19:04:40.789515Z",
     "shell.execute_reply.started": "2025-01-24T19:04:39.228030Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: CometLogger will be initialized in online mode\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import CometLogger\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"COMET_API_KEY\")\n",
    "\n",
    "comet_logger = CometLogger(\n",
    "    api_key=secret_value_0,\n",
    "    project_name=f'UNet_Inpainting-{GROUP_ID}',\n",
    "    workspace='wikiart-inpainting',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T19:04:40.792875Z",
     "iopub.status.busy": "2025-01-24T19:04:40.792381Z",
     "iopub.status.idle": "2025-01-24T19:04:40.806818Z",
     "shell.execute_reply": "2025-01-24T19:04:40.805596Z",
     "shell.execute_reply.started": "2025-01-24T19:04:40.792826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class WikiArtDataset(Dataset):\n",
    "    def __init__(self, h5_path: str, mask_h5_path: str, csv_path: str, set_type: str, group_id=None, label_col='cluster_label', transform=None):\n",
    "        self.h5_path = h5_path\n",
    "        self.mask_h5_path = mask_h5_path\n",
    "        \n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.df = self.df[self.df['set_type'] == set_type]\n",
    "\n",
    "        if group_id is not None:\n",
    "            self.df = self.df[self.df['cluster_label'] == group_id]\n",
    "        \n",
    "        self.label_col = label_col\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.length = len(self.df)\n",
    "  \n",
    "        with h5py.File(self.mask_h5_path, 'r') as mask_h5f:\n",
    "            self.num_masks = mask_h5f['mask'].shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def _open_hdf5(self):\n",
    "        if not hasattr(self, '_hf') or self._hf is None:\n",
    "            self._hf = h5py.File(self.h5_path, 'r')\n",
    "\n",
    "        if not hasattr(self, '_mask_hf') or self._mask_hf is None:\n",
    "            self._mask_hf = h5py.File(self.mask_h5_path, 'r')\n",
    "\n",
    "    def _get_random_mask(self):\n",
    "        mask_idx = np.random.randint(0, self.num_masks)\n",
    "        mask = self._mask_hf['mask'][mask_idx]\n",
    "        return mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        self._open_hdf5()\n",
    "\n",
    "        row = self.df.iloc[idx]\n",
    "        image_idx = row['h5_index']\n",
    "\n",
    "        label = row[self.label_col]\n",
    "\n",
    "        image = self._hf['image'][image_idx]\n",
    "        image = torch.from_numpy(image).float()\n",
    "\n",
    "        mask = self._get_random_mask()\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T19:04:40.808775Z",
     "iopub.status.busy": "2025-01-24T19:04:40.808215Z",
     "iopub.status.idle": "2025-01-24T19:04:40.834581Z",
     "shell.execute_reply": "2025-01-24T19:04:40.833206Z",
     "shell.execute_reply.started": "2025-01-24T19:04:40.808717Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UNetInpainting(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=3, use_dropout=False):\n",
    "        super().__init__()\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "        self.encoder1 = self.conv_block(in_channels, 16)\n",
    "        self.encoder2 = self.conv_block(16, 32, pool=True)\n",
    "        self.encoder3 = self.conv_block(32, 64, pool=True)\n",
    "        self.encoder4 = self.conv_block(64, 128, pool=True)\n",
    "\n",
    "        self.bottleneck = self.conv_block(128, 256, pool=True)\n",
    "\n",
    "        self.decoder4 = self.upconv_block(256, 128)\n",
    "        self.decoder3 = self.upconv_block(128, 64)\n",
    "        self.decoder2 = self.upconv_block(64, 32)\n",
    "        self.decoder1 = self.upconv_block(32, 16)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(16, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels, pool=False):\n",
    "        layers = []\n",
    "        if pool:\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "        layers.extend([\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        ])\n",
    "        if self.use_dropout:\n",
    "            layers.append(nn.Dropout(0.5))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x_with_mask = x * (1 - mask)\n",
    "\n",
    "        x_with_mask_and_mask = torch.cat([x_with_mask, mask], dim=1)\n",
    "\n",
    "        e1 = self.encoder1(x_with_mask_and_mask)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        e4 = self.encoder4(e3)\n",
    "\n",
    "        b = self.bottleneck(e4)\n",
    "\n",
    "        d4 = self.decoder4(b) + e4\n",
    "        d3 = self.decoder3(d4) + e3\n",
    "        d2 = self.decoder2(d3) + e2\n",
    "        d1 = self.decoder1(d2) + e1\n",
    "\n",
    "        output = self.final_conv(d1)\n",
    "\n",
    "        output = output * mask + x * (1 - mask)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T19:04:40.836231Z",
     "iopub.status.busy": "2025-01-24T19:04:40.835903Z",
     "iopub.status.idle": "2025-01-24T19:04:40.855185Z",
     "shell.execute_reply": "2025-01-24T19:04:40.853898Z",
     "shell.execute_reply.started": "2025-01-24T19:04:40.836185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def log_images_to_comet(logger, images, mask, output, epoch, step, name='sample'):\n",
    "    grid = vutils.make_grid(\n",
    "        [images[0].cpu(), (images[0] * (1 - mask[0])).cpu(), output[0].cpu()],\n",
    "        nrow=3,\n",
    "        normalize=True\n",
    "    )\n",
    "\n",
    "    grid_np = grid.permute(1, 2, 0).detach().numpy()  # CxHxW -> HxWxC\n",
    "\n",
    "    grid_image = Image.fromarray((grid_np * 255).clip(0, 255).astype('uint8'))\n",
    "\n",
    "    logger.experiment.log_image(\n",
    "        grid_image, name=f'{name}_epoch_{epoch}_step_{step}.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T19:04:40.860122Z",
     "iopub.status.busy": "2025-01-24T19:04:40.858876Z",
     "iopub.status.idle": "2025-01-24T19:04:40.878057Z",
     "shell.execute_reply": "2025-01-24T19:04:40.876473Z",
     "shell.execute_reply.started": "2025-01-24T19:04:40.860075Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UNetInpaintingLightning(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        return self.model(x, mask)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, mask, _ = batch\n",
    "\n",
    "        mask = mask.unsqueeze(1)\n",
    "\n",
    "        optimizer = self.optimizers()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            output = self(x, mask)\n",
    "            loss = self.criterion(output, x)\n",
    "\n",
    "        self.manual_backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        self.log('learning_rate', current_lr, prog_bar=True, on_epoch=True, on_step=True)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True, on_step=True)\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            log_images_to_comet(\n",
    "                self.logger,\n",
    "                x,\n",
    "                mask,\n",
    "                output,\n",
    "                epoch=self.current_epoch,\n",
    "                step=self.global_step,\n",
    "                name='train_sample'\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, mask, _ = batch\n",
    "        mask = mask.unsqueeze(1)\n",
    "        output = self(x, mask)\n",
    "        loss = self.criterion(output, x)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True, on_step=True)\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            log_images_to_comet(\n",
    "                self.logger,\n",
    "                x,\n",
    "                mask,\n",
    "                output,\n",
    "                epoch=self.current_epoch,\n",
    "                step=self.global_step,\n",
    "                name='val_sample'\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss',\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_loss = self.trainer.callback_metrics.get('val_loss')\n",
    "        if val_loss is not None:\n",
    "            # Step the scheduler manually with val_loss\n",
    "            lr_scheduler = self.lr_schedulers()\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step(val_loss)\n",
    "\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "\n",
    "                weights_path = f'best_weights_epoch_{self.current_epoch}_val_loss_{val_loss:.4f}.pth'\n",
    "                torch.save(self.model.state_dict(), weights_path)\n",
    "\n",
    "                self.logger.experiment.log_model(\n",
    "                    name='best_weights',\n",
    "                    file_or_folder=weights_path,\n",
    "                    overwrite=True\n",
    "                )\n",
    "                print(f'Weights saved and logged to CometML: {weights_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T19:04:40.879933Z",
     "iopub.status.busy": "2025-01-24T19:04:40.879545Z",
     "iopub.status.idle": "2025-01-24T19:04:41.038026Z",
     "shell.execute_reply": "2025-01-24T19:04:41.036889Z",
     "shell.execute_reply.started": "2025-01-24T19:04:40.879881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transforms = v2.Compose([\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    # v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "h5_path = '/kaggle/input/wikiart-clean-without-split/dataset.h5'\n",
    "mask_path = '/kaggle/input/wikiart-damage-masks/square.h5'\n",
    "csv_path = '/kaggle/input/wikiart-cluster-annotations-split/combined_clusters.csv'\n",
    "\n",
    "train_dataset = WikiArtDataset(\n",
    "    h5_path=h5_path,\n",
    "    mask_h5_path=mask_path,\n",
    "    csv_path=csv_path,\n",
    "    set_type='train',\n",
    "    group_id=GROUP_ID,\n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "valid_dataset = WikiArtDataset(\n",
    "    h5_path=h5_path,\n",
    "    mask_h5_path=mask_path,\n",
    "    csv_path=csv_path,\n",
    "    set_type='valid',\n",
    "    group_id=GROUP_ID,\n",
    "    transform=transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T19:04:41.040080Z",
     "iopub.status.busy": "2025-01-24T19:04:41.039703Z",
     "iopub.status.idle": "2025-01-24T19:04:41.046627Z",
     "shell.execute_reply": "2025-01-24T19:04:41.045051Z",
     "shell.execute_reply.started": "2025-01-24T19:04:41.040045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = UNetInpainting(in_channels=4)\n",
    "\n",
    "model.load_state_dict(torch.load('/kaggle/input/wikiart-inpainting-full-set-weights/best_weights_epoch_28_val_loss_3.7235-2348.pth'))\n",
    "\n",
    "checkpoint_path = ''\n",
    "\n",
    "if checkpoint_path == '':\n",
    "    lightning_model = UNetInpaintingLightning(model)\n",
    "else:\n",
    "    lightning_model = UNetInpaintingLightning.load_from_checkpoint(checkpoint_path, model=model)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    dirpath='./checkpoints',\n",
    "    filename='best_model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.00,\n",
    "    patience=8,\n",
    "    verbose=True,\n",
    "    mode='min',\n",
    "    check_on_train_epoch_end=False\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=comet_logger,\n",
    "    max_epochs=30,\n",
    "    devices=1 if torch.cuda.is_available() else 0,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(lightning_model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "comet_logger.experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6434396,
     "sourceId": 10406837,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6435714,
     "sourceId": 10413150,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6435274,
     "sourceId": 10451124,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6469340,
     "sourceId": 10451153,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
